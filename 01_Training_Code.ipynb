{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2104c-9251-411c-b6c5-04917781c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf ## pip install tensorflow\n",
    "import cv2 ## pip install opencv-contrib-python\n",
    "import numpy as np ## pip install numpy\n",
    "import matplotlib.pyplot as plt  ## pip install matplotlib\n",
    "import os\n",
    "!set KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ed976-aea2-492b-a61e-1fb0fa43c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datadirectory = \"Training\" ##training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2923fc-a6e2-42da-8067-b44e815660a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"] ##List of classes=> exact name of your folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e78796-505c-435a-bd07-7636e29705aa",
   "metadata": {},
   "source": [
    "## Testing By Printing a single image from folder '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6445efe-87b4-4570-9836-7dc33c03aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = cv2.imread(\"Training/0/Training_3908.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615a2ab-d963-4092-bbb4-69b89a48289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array.shape #rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c37b9-0795-45ef-999e-c317a8813e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153db113-4dce-4226-b5c0-f5aeeb8b4274",
   "metadata": {},
   "source": [
    "## Preprocessing Demo on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20483167-812b-4564-b934-50f2c78141d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in Classes:\n",
    "    path = os.path.join(Datadirectory,category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img))\n",
    "        #backtorgb = cv2.cvtColor(img_array,cv2.COLOR_GRAY2RGB)\n",
    "        plt.imshow(cv2.cvtColor(img_array,cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00f6f2-a13e-46c0-b966-09aae0db8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=224 ##ImageNet => 224x224\n",
    "new_array=cv2.resize(img_array, (img_size,img_size))\n",
    "plt.imshow(cv2.cvtColor(new_array,cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811d537-3bd6-4a11-9713-99313cc3cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array.shape\n",
    "## Now the image size is changed as needed for the input into ImageNet CNN Architecture that we are gonna use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545eaea7-9b33-4f47-b09c-88da759a0ca8",
   "metadata": {},
   "source": [
    " # Read all images and convert them to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd423d-105a-409c-a260-a4f3872066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_Data=[] ##data\n",
    "\n",
    "def create_training_Data():\n",
    "    for category in Classes:\n",
    "        path = os.path.join(Datadirectory,category)\n",
    "        class_num = Classes.index(category)  ## 0 1,  ## Label\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))\n",
    "                new_array = cv2.resize(img_array, (img_size,img_size))\n",
    "                training_Data.append( [new_array,class_num] )\n",
    "            except Exception as e:\n",
    "                pass\n",
    "               \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ee59e-1d4c-4c79-bcd4-35f796dcddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_Data()\n",
    "print(len(training_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d895be7-9b22-40d7-97fb-0227bb689488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random ## shuffling the images into random order\n",
    "random.shuffle(training_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6ec7c-40fb-4f30-aafd-510c68d5ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [] ## data/feature\n",
    "y = [] ## label\n",
    "for features,label in training_Data:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "x = np.array(x).reshape(-1, img_size, img_size, 3) ## converting it to 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e9597-e26d-4ea5-83a5-3d98d3332d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape  ## Changing the dimension because the deep learning architecture such as mobileNet requires 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47d539-b0bc-493a-8386-e057710b4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "X = x/255.0\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0d151-f869-4ec6-8c3f-5584f7aecfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196032a-9764-484c-8e6e-482216601891",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(y)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af45bc4-efd9-4c6b-8095-2edc7282038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Count the occurrences of each value\n",
    "counts = Counter(Y)\n",
    "\n",
    "# Print the counts\n",
    "for value, count in counts.items():\n",
    "    print(f\"Value {value}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50780ac1-bcdb-4986-bf70-7cd6cbb2da6d",
   "metadata": {},
   "source": [
    "# deep Learning model for training - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcd5f3-b281-4af1-b353-14e068abd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6315794-7f3a-48b4-9388-4947e8350fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.MobileNetV2() ## Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d32de-02c9-41c9-af41-8ef877eaf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df9767-e188-4ccf-ad88-fd63eb5b8095",
   "metadata": {},
   "source": [
    "# Transfer Learning - Tuning, weights will start from last check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15c76f-cb73-4258-8961-2b959f0b3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input = model.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6a277-0d91-461a-acf4-549c0077692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142717cb-7da1-4bf7-b68c-adf59b0ff25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ouput = model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc85e51-99b4-4646-ab22-7119f29a5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa442a-d1fe-4988-8cc9-c34ef7ad7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = layers.Dense(128)(base_ouput) ## adding new layer after output of global pooling layer\n",
    "final_ouput  = layers.Activation('relu')(final_output) ## activation function\n",
    "final_output = layers.Dense(64)(final_ouput)\n",
    "final_ouput  = layers.Activation('relu')(final_output)\n",
    "final_output = layers.Dense(7,activation='softmax')(final_ouput) ## we need 7 classes ## Classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923d547-e41c-4964-8fb5-01c2eaa5e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df74ed3c-10ff-4fd2-87d0-2887835c097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "new_model = keras.Model(inputs = base_input, outputs = final_output)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05abf88-68da-4150-8995-4be8f9495baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer= \"adam\", metrics = [\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7534b1-691a-4019-9120-a3676f25a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(X, Y, epochs=30)\n",
    "new_model.save('fer_model_2.h5')\n",
    "## Training is completed and model saved as a file names 'fer_model_2.h5' in base directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815c90a-565f-4ff1-a472-73d7042a1b9c",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0802f-cd02-4384-b323-f288988416d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('fer_model.h5')  ## Load the model saved after training from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59278fc-4296-49d3-b646-fe36d33faad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(\"happyboy.jpg\")\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222eaea-81c9-4aea-9b07-3288001d3ba6",
   "metadata": {},
   "source": [
    "## we need face detection algorithm (gray image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38bacd-c605-46c2-8380-675b53be8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97ccee-26bc-4307-8237-815a7fd22596",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb3064-c9b1-4532-ae67-88c747134d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4c1a0-dcd7-4762-86c6-3448e69d46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "for x,y,w,h in faces:\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = frame[y:y+h, x:x+w]\n",
    "    cv2.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "    facess = faceCascade.detectMultiScale(roi_gray)\n",
    "    if len(facess) == 0:\n",
    "        print(\"Face not detected\")\n",
    "    else:\n",
    "        for (ex,ey,ew,eh) in facess:\n",
    "            face_roi = roi_color[ey: ey+eh, ex:ex+ew] ## Cropping the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e1aa8-27b5-4aa1-b482-446bb71282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332d308-28a0-4e52-ad65-2a5cfb64ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( cv2.cvtColor(face_roi,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5cd50e-9dc7-4b0c-a629-ad869c5366f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image = cv2.resize(face_roi,(224,224)) ## Converting to 224x224\n",
    "final_image = np.expand_dims(final_image,axis=0) ## need fourth dimension\n",
    "final_image = final_image/255.0 ## normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ef3f0-4481-4991-87f0-7679efe5da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = new_model.predict(final_image)\n",
    "Predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa47002-ffc8-44b1-a067-f49dd1a74d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(Predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
